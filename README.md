# Max Roald Eckardt

I am Senior Software Engineer at the [Center for Hybrid Intelligence](https://mgmt.au.dk/center-for-hybrid-intelligence/). See my [CV on LinkedIn](https://www.linkedin.com/in/max-roald-eckardt-69706071/).

I will use these pages as a blog for thoughts on topics close to my interests. 

## The State of- and Outlook on Agentic AI and Symbolic Reasoning
Yann LeCun excalaimed that he were "no longer interested in LLMs". Here is my take on why that is and what my personal approach to a low-tech solution would be.

GPT-based LLMs solved language. On top of that they offer spurious reasoning. However many agentic AI architectures represent flirting with insanity. LLMs prioritize completion and syntactical integrity over reasoning and factual integrity. LRMs are inefficient improvised architectures accumulating the small reasoning capacity in meshed LLM runs with ok results and poor scaling. The challenge is to manage complexity by breaking it down into delegatable, manageable, and validatable (controllable) tasks and steps. GPTs specialization is language generation, not abstraction thinking in that language.

The solution for abstract thinking is symbolic reasoning with a world model. A world model connecting specific knowledge to abstract reasoning symbols. These connections could be aligned with _reasoning pathways_ fx. 'physical objects are governed by physical laws', 'mental models apply to social beings', and countless others of varying degrees of abstraction. This alignment with chain-of-thought-derived associations offer many benefits. Reasoning and even planning then become hyper-efficient, explainable, test-time informed, and can be achieved with classical graph querying strategies on cheap hardware. Such stragies I already generate dynamically with LRMs for my daily work. 

The generation and maintenance of online layers of a world model is computationally expensive. Taking advantage of the indexicality of language in LLMs a LRM-informed Neural Network (LRMINN) could be set up to process a set of lemmas (fx. the english language) across a limited set of abstractions with anchor terms for human intelligibility to produce an economic baseline for a world model. 

The crux in knowledge indexing is memory management, because the ground truth in social contexts is neither static nor objective. This poses data quality challenges with some remedy found in abstract projections. The planted anchor terms link to key lemmas and symbols to afford generatl explainability in the form of browseable resasoning paths and validatable generation. 

World models will effortlessly accommodate individual users with subtle intricate biases in personalized services invigorating implications for ethical AI use. 

## Vibe Coding vs. Agentic Stack Maintenance
Vibe coding uses LRM for code generation. Due to above limitations to LRM prompt complexity (and other issues) it is not applicable to maintain projects above a (very low) maturity. I have developed agentic strategies with custom semantic index with feathered-abstractions and symbol/reference resolution as a tech demo. This is equivalent to a mini world model for a hyper-formalized code base. It looks over-engineered, but to limit complexity to a proactical degree I had to formulate dozens of design patterns, paradigms, best-practices, and preferences.
